{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: 古诗词生成模型\n",
    "Author: rainym00d, Ethan00Si\n",
    "Github: https://github.com/rainym00d, https://github.com/Ethan00Si\n",
    "Date: 2021-05-07 13:10:00\n",
    "LastEditors: rainym00d\n",
    "LastEditTime: 2021-05-09 13:08:08\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        '''\n",
    "        ***********请在此写入你的代码**********\n",
    "        定义模型：\n",
    "        1. 定义模型隐藏层维度, hidden_dim为读入的隐藏层维度\n",
    "        2. 使用词嵌入表示(word embedding), embedding_dim为读入的嵌入向量的维度\n",
    "        3. 定义LSTM模型（推荐使用2层LSTM）\n",
    "        4. 定义线性模型（从 hidden_dim 映射到 vocab_size）\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.embedding_dim=embedding_dim#定义嵌入层维度\n",
    "        self.hidden_dim=hidden_dim#定义模型隐藏层的维度\n",
    "        self.embeddings=nn.Embedding(vocab_size,self.embedding_dim)#定义词嵌入表示：\n",
    "        #输入的词维度；维度为vocab_size维，即1.2万余维\n",
    "        #输出的词嵌入向量维度：为self.embedding_dim维\n",
    "        self.rnn=nn.LSTM(self.embedding_dim,self.hidden_dim,2)#定义LSTM模型；使用两层LSTM\n",
    "        self.output=nn.Linear(self.hidden_dim,vocab_size)#古诗生成case中，每个字即为一类，所以分类数为vocab_size\n",
    "        #npz古诗数据集中，共有字30余万个，但是不重复字数为1.2万余个：1.2万即vocab_size，即线性层的输出分类数\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        '''\n",
    "        *********请在此处输入你的代码*********\n",
    "        输入：input, 它的size是(sequence_length, batch_size)\n",
    "        输出（返回值）：output(预测值)，hidden(隐藏层的值)\n",
    "            * output的size是(sequence_length, batch_size, vocab_size)\n",
    "            * hidden的size是(4, batch_size, hidden_size)\n",
    "                * h_0 (2, batch_size, hidden_size)\n",
    "                * c_0 (2, batch_size, hidden_size)\n",
    "        定义模型函数：\n",
    "            * 判断输入的参数是否有hidden，没有的话新建一个全0的tensor\n",
    "            * 将input进行词向量嵌入\n",
    "            * 使用lstm模型\n",
    "            * 用线性层将output映射到vocab_size的维度上\n",
    "            * 返回output, hidden\n",
    "        '''\n",
    "\n",
    "        # 以下是2层LSTM所需要的状态初始化(包括h_0和c_0)代码\n",
    "        seq_len, batch_size = input.size()#input是 一个shape为（序列长度，batch_size, embedding_size）的tensor\n",
    "        #seq_len:序列长度【输入的字的个数】；batch_size：分批操作，并行运算的诗/诗句数量\n",
    "        if hidden is None:\n",
    "            h_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "            #LSTM的输出的隐藏值hidden是一个元组：（h_0, c_0 ）——分别为隐状态向量和长时记忆单元向量组成的\n",
    "        # 请在下面补充forward函数的其它代码\n",
    "        input_embeddings=self.embeddings(input)#对输入的input进行词向量嵌入\n",
    "        hiddens, hidden=self.rnn(input_embeddings, (h_0, c_0))#接收上一轮的隐藏值（h_0，c_0）\n",
    "        #hiddens是积累的所有的历史隐藏值：所有隐状态向量h组成的tensor；一个shape为（序列长度, batch_size, hidden_size）的tensor\n",
    "        #hidden是上一轮的隐藏值：最后一个隐状态向量；一个shape为（2，batch_size, hidden_size）的tensor\n",
    "        output=self.output(hiddens)\n",
    "        #线性层输出分类预测结果\n",
    "        #output为一个（seq_len,batch_size,vocab_size）的tensor\n",
    "\n",
    "        return  output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        创建模型和优化器，初始化线性模型和优化器超参数\n",
    "        * 参数\n",
    "            * learning_rate\n",
    "            * epoches\n",
    "            * model_save_path: 模型保存路径\n",
    "            * device: cuda or cpu\n",
    "        * 模型\n",
    "            * 创建PoetryModel的实例, 命名为model\n",
    "            * 定义optimizer\n",
    "            * 定义loss function\n",
    "        \"\"\"\n",
    "        self.lr = 1e-3  # 学习率\n",
    "        self.epoches = 1000+1  # 训练epoch数量\n",
    "        self.model_save_path = './model/PoetryModel'  # 模型保存路径\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 指定训练的device，优先使用GPU，GPU不可用时加载CPU\n",
    "\n",
    "        self.model = PoetryModel(vocab_size, embedding_dim, hidden_dim).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _save_model(self, epoch):\n",
    "        \"\"\"\n",
    "        保存模型，用于训练时保存指定epoch的模型\n",
    "        \"\"\"\n",
    "        print('[INFO] Saving to %s_%s.pth' % (self.model_save_path, epoch))\n",
    "        torch.save(self.model.state_dict(), '%s_%s.pth' % (self.model_save_path, epoch))\n",
    "\n",
    "    def _load_model(self, epoch):\n",
    "        \"\"\"\n",
    "        加载模型，用于加载指定epoch的模型。\n",
    "        目前代码中没有用到。\n",
    "        可以在训练到一半但中断了之后，自行修改代码，从最近的epoch加载，然后继续训练，以节省时间。\n",
    "        或者训练完毕后，下次再跑程序，就直接加载模型，省去训练时间。\n",
    "        \"\"\"\n",
    "        print('[INFO] Loading from %s_%s.pth' % (self.model_save_path, epoch))\n",
    "        self.model.load_state_dict(torch.load('%s_%s.pth' % (self.model_save_path, epoch), map_location=self.device))\n",
    "\n",
    "    def train(self, dataloader):\n",
    "        \"\"\"\n",
    "        训练函数\n",
    "        \"\"\"\n",
    "        # 开始训练\n",
    "        for epoch in range(self.epoches):\n",
    "            loss_list = []\n",
    "            for ii, data_ in enumerate(dataloader):\n",
    "                print('epoch {}: batch {}'.format(epoch, ii), end='\\r')\n",
    "                # 训练\n",
    "                data_ = data_.long().transpose(1, 0).contiguous().to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                input_, target = data_[:-1, :], data_[1:, :]\n",
    "                output, _ = self.model(input_)\n",
    "                # permute(0,2,1)是为了让output从(seq_len, batch_size, vocab_size)变成(seq_len, vocab_size, batch_size)\n",
    "                # 与 target的(seq_len, batch_size)对应\n",
    "                output = output.permute(0, 2, 1)\n",
    "                loss = self.loss_function(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "            epoch_loss = sum(loss_list) / len(loss_list)#啥意思？【截至目前epoch的平均loss值吧】\n",
    "            print(\"[INFO] loss of epoch %s: %s\" % (epoch, epoch_loss))\n",
    "\n",
    "            # 保存模型参数\n",
    "            if epoch % 100 == 0:\n",
    "                #每10轮保存一回model参数\n",
    "                self._save_model(epoch)\n",
    "\n",
    "    def test(self, start_words, ix2word, word2ix, max_gen_len=200):\n",
    "        \"\"\"\n",
    "        description: 给定几个词，根据这几个词接着生成一首完整的诗歌\n",
    "        example:\n",
    "            start_words：u'深度学习'\n",
    "            生成：\n",
    "            深度学习书不怪，今朝月下不胜悲。\n",
    "            芒砀月殿春光晓，宋玉堂中夜月升。\n",
    "            玉徽美，玉股洁。心似镜，澈圆珠，金炉烟额红芙蕖。\n",
    "        提示:\n",
    "            一个字一个字的生成诗歌\n",
    "            将start_words一个字一个字的作为input输入模型\n",
    "            第一次输入模型时，hidden为None，之后hidden都是上次的返回结果\n",
    "            当生成的字为'<EOP>'诗歌结束生成\n",
    "        \"\"\"\n",
    "        pre_input='<START>'\n",
    "        hidden=None\n",
    "        poem=[]\n",
    "        poem.append(start_words)\n",
    "        for i in range(max_gen_len):\n",
    "            if i ==0:\n",
    "                input=torch.tensor([word2ix[pre_input]]).view(1,1).long().to(self.device)\n",
    "                output,hidden=self.model(input,hidden)\n",
    "            elif i<= len(start_words):\n",
    "                word=start_words[i-1]\n",
    "                input=torch.tensor([word2ix[word]]).view(1,1).long().to(self.device)\n",
    "                output,hidden=self.model(input,hidden)\n",
    "            else:\n",
    "                top_index = output.data[0].topk(1)[1][0].item()\n",
    "                poem.append(ix2word[top_index])\n",
    "                \n",
    "                input=torch.tensor(top_index).view(1,1).long().to(self.device)\n",
    "                output,hidden=self.model(input,hidden)\n",
    "                if ix2word[top_index]=='<EOP>':\n",
    "                    break   \n",
    "        return  poem\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def acrostic_test(self, start_words, ix2word, word2ix, max_gen_len=200):\n",
    "        \"\"\"\n",
    "        descrption: 生成藏头诗\n",
    "        example:\n",
    "            start_words : u'深度学习'\n",
    "            生成：\n",
    "            深宫新月明，皎皎明月明。\n",
    "            度风飘飖飏，照景澄清明。\n",
    "            学化不可夺，低心不可怜。\n",
    "            习人不顾盼，仰面空踟蹰。\n",
    "        提示:\n",
    "            与上一个函数类似，但需要特殊处理一下“藏头”\n",
    "            一句结束，即生成“。”或“!”时，诗歌的下一个字为读入的“藏头”。\n",
    "            此时模型的读入为“藏头”对应的字，其他情况下模型读入的是上次生成的字。\n",
    "            当所有“藏头”都生成了一句诗，诗歌生成完毕。\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        lent = len(start_words)\n",
    "        input = torch.Tensor([word2ix['<START>']]).view(1, 1).long().contiguous().to(self.device)\n",
    "        hidden = None\n",
    "\n",
    "\n",
    "        index = 0  # 指示已生成了多少句\n",
    "        pre_word = '<START>'  # 上一个词\n",
    "\n",
    "        # 生成藏头诗\n",
    "        for i in range(max_gen_len):\n",
    "            output, hidden = self.model(input, hidden)\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            w = ix2word[top_index]\n",
    "\n",
    "            # 如果遇到标志一句的结尾，喂入下一个“头”\n",
    "            if (pre_word in {'。', '！','<START>'}):\n",
    "                # 如果生成的诗已经包含全部“头”，则结束\n",
    "                if index == lent:\n",
    "                    break\n",
    "                # 把“头”作为输入喂入模型\n",
    "                else:\n",
    "                    w = start_words[index]\n",
    "                    index += 1\n",
    "                    input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "\n",
    "            # 否则，把上一次预测作为下一个词输入\n",
    "            else:\n",
    "                input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "            results.append(w)\n",
    "            pre_word = w\n",
    "            input=input.contiguous().to(self.device)\n",
    "\n",
    "        return results\n",
    "    \n",
    "        # test_input = torch.tensor([word2ix[char] for char in start_words])\n",
    "#         pre_input='<START>'\n",
    "#         hidden = None\n",
    "#         poem = []\n",
    "#         start_ls=list(start_words)\n",
    "#         for i in range(max_gen_len):\n",
    "#             if i ==0:\n",
    "#                 input=torch.tensor([word2ix[pre_input]]).view(1,1).long().to(self.device)\n",
    "#                 output,hidden=self.model(input,hidden)\n",
    "#             elif (i==1) or (top_index == word2ix['!']) or (top_index == word2ix['。']):\n",
    "#                 try:\n",
    "#                     word=start_ls.pop(0)\n",
    "#                     poem.append(word)\n",
    "#                     input=torch.tensor([word2ix[word]]).view(1,1).long().to(self.device)\n",
    "#                     output,hidden=self.model(input,hidden)\n",
    "#                     top_index=output.data[0].topk(1)[1][0].item()\n",
    "#                     poem.append(ix2word[top_index])\n",
    "#                 except (IndexError):\n",
    "#                     break    \n",
    "#             else:\n",
    "#                 input=torch.tensor([top_index]).view(1,1).long().to(self.device)\n",
    "#                 output,hidden=self.model(input,hidden)\n",
    "#                 top_index=output.data[0].topk(1)[1][0].item()\n",
    "#                 poem.append(ix2word[top_index])\n",
    "                \n",
    "#         return poem\n",
    "\n",
    "# #         #while len(poem)<= max_gen_len:\n",
    "# #         max_len = 100\n",
    "#         i = 0\n",
    "#         for word_vector in start_words:\n",
    "# #             i+=1\n",
    "# #             print(word_vector)\n",
    "# #             print(word2ix[word_vector])\n",
    "\n",
    "# #             initial_input=torch.tensor([word2ix[word_vector]]).view(1,1).long().to(self.device)\n",
    "# #             output, hidden = self.model(initial_input,hidden)\n",
    "# #             top_index= output.data[0].topk(1)[1][0].item()\n",
    "# # #             print(top_index)\n",
    "\n",
    "# #             input=initial_input.data.new([top_index]).view(1,1).to(self.device)\n",
    "# #             #to(self.device)把输入数据存入gpu\n",
    "# #             poem.append(ix2word[top_index])\n",
    "#             poem.append(word_vector)\n",
    "#             input=torch.tensor(word2ix[word_vector]).view(1,1).long().to(self.device)\n",
    "#             top_index=word2ix[word_vector]\n",
    "\n",
    "            \n",
    "            \n",
    "#             while ix2word[top_index] != '。'and ix2word[top_index] != '！':\n",
    "#                 output,hidden =self.model(input,hidden)\n",
    "#                 top_index = output.data[0].topk(1)[1][0].item()\n",
    "#                 input=input.data.new([top_index]).view(1,1).to(self.device)\n",
    "#                 #to(self.device)把输入数据存入gpu\n",
    "#                 poem.append(ix2word[top_index])\n",
    "#                 # print('go')\n",
    "#                 i+=1\n",
    "#                 if(i>max_gen_len):\n",
    "#                     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    return word2ix: dict,每个字对应的序号，形如u'月'->100\n",
    "    return ix2word: dict,每个序号对应的字，形如'100'->u'月'\n",
    "    return poet_data: numpy数组，每一行是一首诗对应的字的下标\n",
    "    \"\"\"\n",
    "    if os.path.isfile(data_path):\n",
    "        data = np.load(data_path, allow_pickle=True)\n",
    "        poet_data, word2ix, ix2word = data['data'], data['word2ix'].item(), data['ix2word'].item()\n",
    "\n",
    "        return word2ix, ix2word, poet_data\n",
    "    else:\n",
    "        print('[ERROR] Data File Not Exists')\n",
    "        exit()\n",
    "\n",
    "\n",
    "def decode_poetry(idx, word2ix, ix2word, poet_data):\n",
    "    \"\"\"\n",
    "    解码诗歌数据\n",
    "    输入:\n",
    "        idx: 第几首诗歌(共311823首，idx in [0, 311822])\n",
    "    \"\"\"\n",
    "    assert (idx < poet_data.shape[0] and idx >= 0)\n",
    "\n",
    "    row = poet_data[idx]\n",
    "\n",
    "    results = ''.join([\n",
    "        ix2word[char] if ix2word[char] != '</s>' and ix2word[char] != '<EOP>'\n",
    "                         and ix2word[char] != '<START>' else ''\n",
    "        for char in row\n",
    "    ])\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 获取数据\n",
    "    data_path = './data/Poetry_data_word2ix_ix2word.npz'\n",
    "    word2ix, ix2word, poet_data = load_data(data_path)\n",
    "\n",
    "    # 测试诗词解码\n",
    "    idx = 1000\n",
    "    poetry = decode_poetry(idx, word2ix, ix2word, poet_data)\n",
    "    print('poetry id: %d\\ncontent: %s' % (idx, poetry))\n",
    "\n",
    "    # 转换为tensor与dataloader\n",
    "    poet_data = poet_data[:1000, ]  # 为测试方便，只截取了前1000条数据训练。后续代码跑通了并且可以用GPU时，可以用全部数据\n",
    "    poet_data = torch.from_numpy(poet_data)\n",
    "    dataloader = data.DataLoader(poet_data,\n",
    "                                 batch_size=128,\n",
    "                                 shuffle=True)\n",
    "\n",
    "    # 定义模型\n",
    "    model = Model(len(word2ix), 128, 256)\n",
    "\n",
    "    # 查看目前使用的是GPU or CPU\n",
    "    print('[INFO] Device Is %s' % model.device)\n",
    "\n",
    "    # 模型训练\n",
    "    model.train(dataloader)\n",
    "\n",
    "    # 测试生成藏头诗\n",
    "    start_words = \"深度学习\"\n",
    "    result = model.acrostic_test(start_words, ix2word, word2ix)\n",
    "    print(''.join(result))\n",
    "\n",
    "    # 测试普通生成诗词\n",
    "    start_words = \"深度学习\"\n",
    "    result = model.test(start_words, ix2word, word2ix)\n",
    "    print(''.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading from ./model/PoetryModel_100.pth\n",
      "[INFO] Device Is cpu\n",
      "清风徐来花，夕色摇阴上。\n",
      "吞门得正棱，雪为菰翠午。\n",
      "芳创自自娱，幽物终有客。\n",
      "谁知公子来，亦有千里者。\n",
      "萧萧洒木泉，古木虫鸣宿。\n",
      "主人樵夫人，涕叟嗟我辱。\n",
      "顾我守渊市，得予事贱倒。\n",
      "人间我我师，我亦何况汝。\n",
      "况复哀子山，其也不敢酒。\n",
      "哀心在君日，犹是金石路。\n",
      "但觉雨声游，纵横抱新厅。\n",
      "芭苔谢秋中，寒食无一斑。\n",
      "此意若得谢，此意若可叹。\n",
      "\n",
      "        清风徐来        \n",
      "清光何可二，二十十二数。\n",
      "风如如我去，万古如不足。\n",
      "徐岂无故宫，岂可顾贪宅。\n",
      "来子不得其，一笑成绝迹。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 获取数据\n",
    "data_path = './data/Poetry_data_word2ix_ix2word.npz'\n",
    "word2ix, ix2word, poet_data = load_data(data_path)\n",
    "def decode_test(idx = 1000):\n",
    "    # 测试诗词解码\n",
    "    idx = 1000\n",
    "    poetry = decode_poetry(idx, word2ix, ix2word, poet_data)\n",
    "    print('poetry id: %d\\ncontent: %s' % (idx, poetry))\n",
    "\n",
    "    # 转换为tensor与dataloader\n",
    "poet_data = poet_data[:10000, ]  # 为测试方便，只截取了前1000条数据训练。后续代码跑通了并且可以用GPU时，可以用全部数据\n",
    "poet_data = torch.from_numpy(poet_data)\n",
    "dataloader = data.DataLoader(poet_data,\n",
    "                             batch_size=128,\n",
    "                             shuffle=True)\n",
    "\n",
    "# 定义模型\n",
    "model = Model(len(word2ix), 128, 256)\n",
    "model._load_model(epoch=100)\n",
    "# 查看目前使用的是GPU or CPU\n",
    "print('[INFO] Device Is %s' % model.device)\n",
    "\n",
    "# # 模型训练\n",
    "#model.train(dataloader)\n",
    "\n",
    "start_words = \"清风徐来\"\n",
    "# 测试普通生成诗词\n",
    "#start_words = \"深度学习\"\n",
    "result = model.test(start_words, ix2word, word2ix)\n",
    "print(''.join(result).replace('。','。\\n').replace('<EOP>',''))\n",
    "\n",
    "# 测试生成藏头诗\n",
    "#start_words = \"程亮憨憨\"\n",
    "result = model.acrostic_test(start_words, ix2word, word2ix)\n",
    "print('{:^20}'.format(start_words))\n",
    "print(''.join(result).replace('。','。\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poetry id: 1000\n",
      "content: 登高闻旧俗，况此奉尧觞。鸣跸遥空下，层楼颢气傍。御沟新过雨，苑树未经霜。楚泽丹苞重，吴畦紫穗香。三侯天纵妙，九节召南章。中岂矜多艺，威将慑四方。清时人共乐，白日坐间长。近从叨慈渥，区区祝寿昌。\n",
      "[INFO] Device Is cpu\n",
      "epoch 0: batch 0\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-84-aca3e2663c57>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# 模型训练\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m# 测试生成藏头诗\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-d763371cc560>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[1;31m# 与 target的(seq_len, batch_size)对应\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\result_product\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\result_product\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m    916\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\result_product\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2019\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2021\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\result_product\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.84966599999998,
   "position": {
    "height": "40px",
    "left": "1010px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
